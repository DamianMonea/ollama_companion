"""
Author: Damian Monea
Date: 16-01-2024

This is a small Python script that acts as an interface to interact
with an OLlama endpoint.
The Python streams the responses generated by the LLM in real time.
"""

import requests
import json

# Main function that will do everything
def main():
    
    # The URL of our endpoint
    url = "http://localhost:11434/api/chat"

    # We'll use this to store the current prompt from the user.
    prompt = ""

    # We'll use an array to store all of the messages between the user and
    # the LLM in the current session.
    history = []

    # Here we will read the user's input until they enter 'bye'
    while(prompt != "bye"):
        # Read user input
        prompt = input("User => ")

        # Check if the exit command was received
        if prompt == "bye":
            break

        # Append the current message to the history.
        history.append({"role": "user", "content": prompt})

        # The data that will be sent to the OLlama endpoint
        data = {
            "model": "mixtral",
            "messages": history,
            "stream": True
        }

        # Create a new session to stream the model's response
        sess = requests.Session()

        # Print a pretty message for UX purposes
        print("Assistant =>",end="")

        # Stream the model's response and print it, bit by bit.
        with sess.post(url, json=data, stream=True) as resp:
            for line in resp.iter_lines():
                if line:
                    full_response = line.decode("utf-8")
                    bit = json.loads(full_response)
                    print(bit["message"]["content"], end="", flush=True)
        print()

# Call the main function
if __name__ == "__main__":
    main()